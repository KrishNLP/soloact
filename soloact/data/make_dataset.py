import sox
import random
import yaml
import os
import numpy as np
from inspect import getmembers, signature, isclass, isfunction, ismethod
import librosa
import librosa.display
import yaml
import tempfile
import glob
import logging
import pandas as pd
import itertools
import sys
from collections import OrderedDict
from pprint import pprint
import soloact
import traceback
import re
import uuid
import progressbar

random.seed(666) # seed is set for the (hidden) global Random()

def flatten(x, par = '', sep ='.'):
	"""
	Recursively flatten dictionary with parent key separator
	Use to flatten augmentation labels in DataFrame output

	Args:
		x (dict): with nested dictionaries.
		par (str): parent key placeholder for subsequent levels from root
		sep (str: '.', '|', etc): separator

	Returns:
		Flattened dictionary

	Example:
		x = {'Cookies' : {'milk' : 'Yes', 'beer' : 'No'}}
		par, sep = <defaults>
		output = {'Cookies.milk' : 'Yes', 'Cookies.beer' : 'No'}

	"""

	store = {}
	for k,v in x.items():
		if isinstance(v,dict):
			store = {**store, **flatten(v, par =  par + sep + k if par else k)}
		else:
			store[par + sep + k] = v
	return store

def load_track(path, sr = 44100):

	"""Librosa load to numpy array

	Args:
		path (str): filepath
		sr (int): sampling rate - 44100 default and preferred

	"""

	x, sr = librosa.load(path, sr = sr) # default
	return x

def rand(x,y):
	"""

	Randomizer for augmentation pipepline

	Args:
	x (int, float): lower_bound
	y (int, float): upper_bound

	Returns:
		random number between bounds

	"""

	# use uniform if parameters passed are below 1
	if all([v < 1 for v in [x,y]]):
		return random.uniform(x, y)
	else:
		return random.randint(x,y)

def validate_reduce_fx(effects):

	"""
	Function crossvalidating existence of effects
	between pysox and configuration

	Args:
		effects (dict): desired effects for augmentation


	Returns:
		Bool & dict
			True - all effects present, return original effects dictionary
			False - one or more not present, return effects not in pysox

	"""
	# VALIDATE EFFECTS BEFORE STARTING AUGMENTATION CHAIN
	SUPPORTED = ['overdrive', 'chorus', 'reverb', 'phaser']
	FX = sox.Transformer()
	sox_arsenal = dict(getmembers(FX, predicate=lambda x: ismethod(x)))
	requested_effects = {k:v for k,v in effects.get('effects').items() if k in effects.get('active')}
	try:
		assert all(f in sox_arsenal for f in requested_effects), '1'
		assert all(f in SUPPORTED for f in requested_effects), '1'
		pipline_order = ['overdrive'] + [f for f in requested_effects.keys() if \
							f not in ['reverb', 'overdrive']] + ['reverb']
		ordered_effects = OrderedDict.fromkeys(pipline_order)
		for key, value in requested_effects.items():
			ordered_effects[key] = value
		return True, ordered_effects, ''
	except Exception as e:
		# TO DO, custom error classes
		error_message_log = {
				'1':  "Invalid methods provided, what's available: {}"}
		msg_ref = e.__str__()
		if msg_ref == '1':
			invalid_effects = {f for f in requested_effects if f not in sox_arsenal}
			return False, invalid_effects, error_message_log[msg_ref].format(SUPPORTED)
		else:
			return None, None, msg_ref

logger = logging.getLogger()
logger.setLevel('CRITICAL')


def feature_pipeline(arr, feature_kind = 'mfcc'):

	"""
	Current feature pipeline supporting mfcc only

	Args:
		arr(np array): row vector generated by librosa.load
		kwargs (dict): arguments to feature.mfcc

	Returns:
		vector of shape (n_mfcc, )

	"""
	if feature_kind == 'mfcc':
		mfcc = librosa.feature.mfcc(arr,
				sr = 44100,
				n_mfcc = 26)
		mfcc_mean = np.mean(mfcc, axis = 0)
		return mfcc_mean
	elif feature_kind == 'chroma':
		stft = np.abs(librosa.stft(arr))
		mfcc = librosa.feature.chroma_stft(S = stft, sr = 44100).T
		mfcc_mean = np.mean(mfcc, axis = 0)
		return mfcc_mean

def pad(l_arrays):

	"""
	Naive padding using max shape from list of numpy arrays
	to normalize shapes for all

	Args:
		l_array (list of np arrays):

	Returns:
		np.matrix of shape (length of array, features, 1)

	"""
	# Retrieve max
	max_shape = max([x.shape for x in l_arrays], key = lambda x: x[0])

	def padder(inp, max_shape):
		zero_grid = np.zeros(max_shape)
		x,y = inp.shape
		zero_grid[:x, :y] = inp
		return zero_grid

	# Pad with zero grid skeleton
	reshapen = [padder(x, max_shape) for x in l_arrays]

	# Make ndarray
	batch_x = np.array(reshapen)

	return batch_x


def augment_track(file, destination, model_selection, n, effects, feature_kind,
			   exercise = 'regression',
			   sustain = ['overdrive', 'reverb'],
			   write = False
			  ):
	"""
	Track-wise augmentation procedure

	- Classification: randomize effect on or off - no randomization at parameter level
	- Regression: Persistent effects with randomization at parameter level
				  dependent on config state ('default', 'random')

	Args:
		file (str): filepath to .wav file
		effects (dict): candidate effects defined by config state
			state (str):
				'random' : requires upper and lower bounds
				'constant': will take upper if default is False, otherwise effect default
		exercise (str): regression or classification
		sustain: effect to persist despite randomized on/off in classification
		write (bool: False, str: Path):
			Path: if directory not present will make

	"""
	# Init transformer
	FX = sox.Transformer()

	labels = {}

	for effect, parameters in effects.items():

		if exercise.lower() == 'classification' and effect not in sustain:

			# turn effects on or off randomly
			if int(random.choice([True, False])) == 0:
				# print ('{} skipped!'.format(effect))
				continue # skip effect

		effect_f = getattr(FX, effect)
		f_defaults = signature(effect_f).parameters

		# store defaults, could be done out of scope
		f_defaults = {k: f_defaults[k].default for k in f_defaults.keys()}

		used = {}

		for param, val in parameters.items():

			state = val.get('state')
			default = val.get('default') # boolean whether to use default or not

			if state == 'constant':
				used[param] = f_defaults.get(param) if default is True else val.get('upper') # upper can be a list
			elif state == 'random':
				# retrieve bounds
				if not isinstance(f_defaults.get(param), list):
					lower, upper = [val.get(bound) for bound in ['lower', 'upper']]
					assert upper > lower, \
					   'Upper bound for {} must be greater than its lower bound'.format(effect + '.' + param)
					used[param] = rand(lower, upper)
					continue
				raise TypeError('Will not parse random list values!')
		effect_f(**used)
		labels[effect] = used

	record_meta = {}
	record_meta['chord_name'] = file.split('/')[-1]
	model_name = model_selection.search(file)
	record_meta['model_name'] = model_name.group(0) if model_name else ''

	if write is not False:
		outfile = os.path.join(destination.trace, '', record_meta['model_name'] , str(n) + '_' + record_meta['chord_name'])
		FX.build(file, outfile)

	with tempfile.NamedTemporaryFile(suffix = '.wav') as tmp:
		# pysox doesn't have output to array, save to temp file and reload as array with librosa
		FX.build(file, tmp.name)
		sound_array, sr = librosa.load(tmp.name, sr = 41000)
		FX.clear_effects()
		# return data with feature extraction
		flattened_labels = flatten(labels)
		flattened_labels['group'] = n
		flattened_labels = {**flattened_labels, **record_meta}
		return flattened_labels, feature_pipeline(sound_array, feature_kind = 'mfcc')

def augment_data(paths, subsample = False, augment_x = 1, configuration = None,
				 write_augmented = False,
				 write_data = False, feature_extraction = 'mfcc',
				 strategy = 'powers', exercise = 'regression'):
	"""
	Augmentation pipeline with options to subsample or write augmented data

	Args:
		subsample (bool, int): if not False augment only k files
		write_augmented (bool, str: path): if not False write augmented .wav files without feature extraction
		write_data (bool):
			True -> write to data/processed folder with feature extraction (intended to mimic pipleine structure)
			False -> return ndarray of features and dataframe of labels
		augument_x(int): number of augmentations per file

	Returns:
		if no write_with_effects path provided:
			ndarray of features and dataframe of labels
		otherwise:
			files written to provided directory
	"""
	config_path = None
	if not configuration:
		config_path = paths['primary_config']
		print ('Using primary configuration to augment data')
	else:
		config_path = configuration

	config = yaml.load(open(config_path, 'r'))

	augmentation_config = config['DataAugmentation']
	augmentation_config['exercise'] = exercise
	augmentation_config['feature_kind'] = feature_extraction

	valid, parsed_effects, error_message = validate_reduce_fx(augmentation_config)
	if not valid:
		print (error_message)
		print ('Please alter your configuration')
		sys.exit('Stopping..')

	augmentation_config['effects'] = parsed_effects

	print ('----- ACTIVE EFFECTS -----')
	pprint (augmentation_config['active'])
	# augmentation pipline doesn't need this
	augmentation_config.pop('active')

	print ('----- PERSISTING EFFECTS -----')
	pprint (augmentation_config['sustain'])

	models_trained, models_holdout = (config['pipeline_config'].get(x) \
									for x in ['train_models', 'test_models'])
	all_models = models_trained + [models_holdout]

	print ('----- GUITAR MODELS FOR TRAINING -----')
	print ('\n'.join('{} {}'.format(*v) for v in enumerate(models_trained)))
	print ('----- MODELS FOR HOLDOUT -----')
	print ('\n'.join('{} {}'.format(*v) for v in enumerate([models_holdout])))

	chord_paths = paths.get('chord_paths')
	augmented_paths = paths.get('augmentation_paths')

	def path_by_strat(paths_):
		return list(filter(lambda x: x.meta == strategy,paths_))[0]

	soundfile_path = path_by_strat(chord_paths)
	soundfile_path = soundfile_path.trace + soundfile_path.extension
	destination = path_by_strat(paths_ = augmented_paths)
	use_soundfiles = glob.glob(soundfile_path)

	if subsample not in [False, None]:
		print ('Subsampling {} files from {} available'.format(subsample, len(use_soundfiles)))
		use_soundfiles = random.choices(population = use_soundfiles, k = subsample)

	if write_augmented:
		print ('Are you sure you want to augmented {} tracks to "{}"?'.format(
				len(use_soundfiles) * augment_x, destination.trace))

		print ('1 to proceed, any other key to terminate')
		if int(input()) == 1:

			# Build necessary files
			for model in all_models:
				model_paths = os.path.join(destination.trace, model, '')
				os.makedirs(model_paths, exist_ok = True)
			augmentation_config['write'] = destination.trace

		else:
			augmentation_config['write'] = False

	model_pattern = re.compile('|'.join(all_models))

	store_all = []

	print ('\nAUGMENTING DATA, please be patient..')

	with progressbar.ProgressBar(maxval=(len(use_soundfiles) * augment_x))  as bar:
		total = 0
		for ix, sf in enumerate(use_soundfiles):
			for i in range(augment_x):
				store_all.append(augment_track(sf,
									model_selection = model_pattern,
									destination = destination,
									n = i,
									**augmentation_config
									))
				total += 1
				bar.update(total)

	labels, features = zip(*store_all)
	all_features = [np.expand_dims(x, axis = 1) for x in features]

	features = pad(all_features)
	labels = pd.DataFrame(list(labels))

	if write_data is True:
		results_dir = paths.get('reports_path')
		write_dir = os.path.join(results_dir(), '')
		np.save(write_dir  + 'training_X_' + strategy, arr=features)
		labels.to_csv(open(write_dir + 'training_Y_' + strategy  + '.csv', 'w'))
		print ('Features and labels written to "{}"'.format(write_dir))

	return features, labels
